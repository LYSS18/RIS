"""
åŸå§‹LSTMè½¨è¿¹é¢„æµ‹æ¨¡å‹æµ‹è¯•å™¨
æµ‹è¯•åŸå§‹è½¨è¿¹é¢„æµ‹æ¨¡å‹çš„æ€§èƒ½å¹¶ç”Ÿæˆå¯è§†åŒ–ç»“æœ
"""

import os
import sys
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error
import tensorflow as tf
from tensorflow.keras.models import load_model

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..'))

def load_test_data():
    """åŠ è½½æµ‹è¯•æ•°æ®"""
    print("1. åŠ è½½æµ‹è¯•æ•°æ®...")
    
    # åŠ è½½æ•°æ®
    data_path = os.path.join('..', '..', 'data', 'marathon_data_with_features.csv')
    if not os.path.exists(data_path):
        print(f"âŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_path}")
        return None, None, None, None
    
    data = pd.read_csv(data_path)
    print(f"âœ… æ•°æ®åŠ è½½æˆåŠŸï¼Œå…± {len(data)} æ¡è®°å½•")
    
    # æå–ç‰¹å¾
    features = data[['latitude', 'longitude', 'speed', 'direction']].copy()
    
    # æ•°æ®å½’ä¸€åŒ–
    scaler = MinMaxScaler()
    features_scaled = scaler.fit_transform(features)
    
    # åˆ›å»ºåºåˆ—æ•°æ®
    sequence_length = 10
    X, y = [], []
    
    for i in range(sequence_length, len(features_scaled)):
        X.append(features_scaled[i-sequence_length:i])
        y.append(features_scaled[i, :2])  # åªé¢„æµ‹ç»çº¬åº¦
    
    X, y = np.array(X), np.array(y)
    
    # åˆ’åˆ†è®­ç»ƒæµ‹è¯•é›†
    train_size = int(len(X) * 0.8)
    testX = X[train_size:]
    testY = y[train_size:]
    
    print(f"âœ… æµ‹è¯•æ•°æ®å‡†å¤‡å®Œæˆ")
    print(f"   æµ‹è¯•é›†å¤§å°: {testX.shape}")
    
    return testX, testY, scaler, features

def smooth_predictions(predictions, window_size=5):
    """å¯¹é¢„æµ‹ç»“æœè¿›è¡Œç§»åŠ¨å¹³å‡å¹³æ»‘"""
    if len(predictions) < window_size:
        return predictions
    
    smoothed = np.copy(predictions)
    for i in range(window_size, len(predictions)):
        smoothed[i] = np.mean(predictions[i-window_size:i+1], axis=0)
    
    return smoothed

def calculate_metrics(true_coords, pred_coords):
    """è®¡ç®—è¯„ä¼°æŒ‡æ ‡"""
    # è®¡ç®—æ¬§å‡ é‡Œå¾—è·ç¦»è¯¯å·®
    errors = []
    for i in range(len(true_coords)):
        lat_diff = (true_coords[i, 0] - pred_coords[i, 0]) * 111000  # çº¬åº¦è½¬ç±³
        lon_diff = (true_coords[i, 1] - pred_coords[i, 1]) * 111000 * np.cos(np.radians(true_coords[i, 0]))  # ç»åº¦è½¬ç±³
        error = np.sqrt(lat_diff**2 + lon_diff**2)
        errors.append(error)
    
    errors = np.array(errors)
    
    # è®¡ç®—å„ç§æŒ‡æ ‡
    avg_error = np.mean(errors)
    median_error = np.median(errors)
    max_error = np.max(errors)
    std_error = np.std(errors)
    
    # è®¡ç®—ç²¾åº¦æŒ‡æ ‡
    accuracy_30m = np.sum(errors <= 30) / len(errors) * 100
    accuracy_50m = np.sum(errors <= 50) / len(errors) * 100
    accuracy_80m = np.sum(errors <= 80) / len(errors) * 100
    accuracy_100m = np.sum(errors <= 100) / len(errors) * 100
    
    return {
        'errors': errors,
        'avg_error': avg_error,
        'median_error': median_error,
        'max_error': max_error,
        'std_error': std_error,
        'accuracy_30m': accuracy_30m,
        'accuracy_50m': accuracy_50m,
        'accuracy_80m': accuracy_80m,
        'accuracy_100m': accuracy_100m
    }

def calculate_direction_stability(coords):
    """è®¡ç®—æ–¹å‘ç¨³å®šæ€§"""
    if len(coords) < 3:
        return 0.0
    
    directions = []
    for i in range(1, len(coords)):
        lat_diff = coords[i, 0] - coords[i-1, 0]
        lon_diff = coords[i, 1] - coords[i-1, 1]
        direction = np.arctan2(lat_diff, lon_diff)
        directions.append(direction)
    
    # è®¡ç®—æ–¹å‘å˜åŒ–çš„æ ‡å‡†å·®
    direction_changes = []
    for i in range(1, len(directions)):
        change = abs(directions[i] - directions[i-1])
        # å¤„ç†è§’åº¦è·³è·ƒ
        if change > np.pi:
            change = 2*np.pi - change
        direction_changes.append(change)
    
    if len(direction_changes) == 0:
        return 1.0
    
    stability = 1.0 - (np.std(direction_changes) / np.pi)
    return max(0.0, stability)

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ”¬ åŸå§‹LSTMè½¨è¿¹é¢„æµ‹æ¨¡å‹æµ‹è¯•å™¨")
    print("=" * 60)
    
    # 1. åŠ è½½æµ‹è¯•æ•°æ®
    testX, testY, scaler, features = load_test_data()
    if testX is None:
        return
    
    # 2. åŠ è½½åŸå§‹æ¨¡å‹
    print("\n2. åŠ è½½åŸå§‹LSTMæ¨¡å‹...")
    model_path = os.path.join('..', '..', 'models', 'traj_model_120.h5')
    
    if not os.path.exists(model_path):
        print(f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
        print("è¯·å…ˆè¿è¡Œè®­ç»ƒè„šæœ¬ç”Ÿæˆæ¨¡å‹æ–‡ä»¶")
        return
    
    try:
        model = load_model(model_path)
        print(f"âœ… åŸå§‹æ¨¡å‹åŠ è½½æˆåŠŸ: {model_path}")
        print(f"   æ¨¡å‹ç»“æ„: {model.summary()}")
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return
    
    # 3. æ¨¡å‹é¢„æµ‹
    print("\n3. è¿›è¡Œè½¨è¿¹é¢„æµ‹...")
    predicted = model.predict(testX, verbose=0)
    print(f"âœ… é¢„æµ‹å®Œæˆï¼Œé¢„æµ‹äº† {len(predicted)} ä¸ªç‚¹")
    
    # 4. åå½’ä¸€åŒ–
    print("\n4. åå½’ä¸€åŒ–å¤„ç†...")
    coord_scaler = MinMaxScaler()
    coord_scaler.fit(features.iloc[:, :2])
    
    predicted_inverse = coord_scaler.inverse_transform(predicted)
    testY_inverse = coord_scaler.inverse_transform(testY)
    
    # 5. åº”ç”¨å¹³æ»‘å¤„ç†
    print("\n5. åº”ç”¨è½¨è¿¹å¹³æ»‘...")
    predicted_smooth = smooth_predictions(predicted_inverse, 5)
    
    # 6. è®¡ç®—è¯„ä¼°æŒ‡æ ‡
    print("\n6. è®¡ç®—è¯„ä¼°æŒ‡æ ‡...")
    
    # åŸå§‹é¢„æµ‹æŒ‡æ ‡
    metrics_orig = calculate_metrics(testY_inverse, predicted_inverse)
    
    # å¹³æ»‘åæŒ‡æ ‡
    metrics_smooth = calculate_metrics(testY_inverse, predicted_smooth)
    
    # æ–¹å‘ç¨³å®šæ€§
    direction_stability_orig = calculate_direction_stability(predicted_inverse)
    direction_stability_smooth = calculate_direction_stability(predicted_smooth)
    
    # 7. è¾“å‡ºç»“æœ
    print("\n7. æµ‹è¯•ç»“æœåˆ†æ:")
    print("=" * 60)
    
    print(f"\nğŸ“Š åŸå§‹é¢„æµ‹æ€§èƒ½:")
    print(f"  - å¹³å‡è¯¯å·®: {metrics_orig['avg_error']:.2f}m")
    print(f"  - ä¸­ä½æ•°è¯¯å·®: {metrics_orig['median_error']:.2f}m")
    print(f"  - æœ€å¤§è¯¯å·®: {metrics_orig['max_error']:.2f}m")
    print(f"  - è¯¯å·®æ ‡å‡†å·®: {metrics_orig['std_error']:.2f}m")
    print(f"  - 30mç²¾åº¦: {metrics_orig['accuracy_30m']:.2f}%")
    print(f"  - 50mç²¾åº¦: {metrics_orig['accuracy_50m']:.2f}%")
    print(f"  - 80mç²¾åº¦: {metrics_orig['accuracy_80m']:.2f}%")
    print(f"  - 100mç²¾åº¦: {metrics_orig['accuracy_100m']:.2f}%")
    print(f"  - æ–¹å‘ç¨³å®šæ€§: {direction_stability_orig:.3f}")
    
    print(f"\nğŸ“ˆ å¹³æ»‘åæ€§èƒ½:")
    print(f"  - å¹³å‡è¯¯å·®: {metrics_smooth['avg_error']:.2f}m")
    print(f"  - ä¸­ä½æ•°è¯¯å·®: {metrics_smooth['median_error']:.2f}m")
    print(f"  - æœ€å¤§è¯¯å·®: {metrics_smooth['max_error']:.2f}m")
    print(f"  - è¯¯å·®æ ‡å‡†å·®: {metrics_smooth['std_error']:.2f}m")
    print(f"  - 30mç²¾åº¦: {metrics_smooth['accuracy_30m']:.2f}%")
    print(f"  - 50mç²¾åº¦: {metrics_smooth['accuracy_50m']:.2f}%")
    print(f"  - 80mç²¾åº¦: {metrics_smooth['accuracy_80m']:.2f}%")
    print(f"  - 100mç²¾åº¦: {metrics_smooth['accuracy_100m']:.2f}%")
    print(f"  - æ–¹å‘ç¨³å®šæ€§: {direction_stability_smooth:.3f}")
    
    # 8. æ€§èƒ½è¯„ä¼°
    print(f"\nğŸ¯ æ€§èƒ½è¯„ä¼°:")
    if metrics_smooth['avg_error'] < 40:
        print("  - é«˜ç²¾åº¦æ¨¡å‹ï¼šè¯¯å·®å°äº40m âœ…")
    elif metrics_smooth['avg_error'] < 60:
        print("  - è‰¯å¥½ç²¾åº¦æ¨¡å‹ï¼šè¯¯å·®å°äº60m")
    else:
        print("  - éœ€è¦æ”¹è¿›ï¼šè¯¯å·®è¾ƒå¤§")
    
    if metrics_smooth['accuracy_50m'] > 80:
        print("  - 50mç²¾åº¦è¡¨ç°ä¼˜ç§€ âœ…")
    elif metrics_smooth['accuracy_50m'] > 70:
        print("  - 50mç²¾åº¦è¡¨ç°è‰¯å¥½")
    
    if direction_stability_smooth > 0.8:
        print("  - è½¨è¿¹æ–¹å‘ç¨³å®šæ€§ä¼˜ç§€ âœ…")
    elif direction_stability_smooth > 0.7:
        print("  - è½¨è¿¹æ–¹å‘ç¨³å®šæ€§è‰¯å¥½")
    
    # 9. ç”Ÿæˆå¯è§†åŒ–ç»“æœ
    print("\n9. ç”Ÿæˆå¯è§†åŒ–ç»“æœ...")
    
    # åˆ›å»ºè½¨è¿¹å¯¹æ¯”å›¾
    plt.figure(figsize=(12, 8))
    
    # è½¨è¿¹å¯¹æ¯”å›¾ - æ˜¾ç¤ºå‰1000ä¸ªç‚¹ä»¥ä¾¿æ¸…æ™°æ˜¾ç¤º
    sample_size = min(1000, len(testY_inverse))
    plt.plot(testY_inverse[:sample_size, 1], testY_inverse[:sample_size, 0],
             'g-', label='çœŸå®è½¨è¿¹', linewidth=3, alpha=0.8)
    plt.plot(predicted_smooth[:sample_size, 1], predicted_smooth[:sample_size, 0],
             'r-', label='é¢„æµ‹è½¨è¿¹', linewidth=2, alpha=0.8)
    
    # æ·»åŠ èµ·ç‚¹å’Œç»ˆç‚¹æ ‡è®°
    plt.plot(testY_inverse[0, 1], testY_inverse[0, 0], 'go', markersize=10, label='èµ·ç‚¹')
    plt.plot(testY_inverse[sample_size-1, 1], testY_inverse[sample_size-1, 0], 'rs', markersize=10, label='ç»ˆç‚¹')
    
    plt.title(f'åŸå§‹LSTMè½¨è¿¹é¢„æµ‹å¯¹æ¯” (å¹³å‡è¯¯å·®: {metrics_smooth["avg_error"]:.1f}m, å‰{sample_size}ç‚¹)', 
              fontsize=16, fontweight='bold')
    plt.xlabel('ç»åº¦', fontsize=14)
    plt.ylabel('çº¬åº¦', fontsize=14)
    plt.legend(fontsize=12)
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    results_path = os.path.join('..', '..', 'results', 'original_model_results.png')
    plt.savefig(results_path, dpi=300, bbox_inches='tight')
    plt.show()
    
    print("âœ… æµ‹è¯•å®Œæˆï¼")
    print(f"ğŸ“Š å¯è§†åŒ–ç»“æœå·²ä¿å­˜ä¸º {results_path}")
    print("=" * 60)
    print("ğŸ¯ åŸå§‹LSTMæ¨¡å‹æµ‹è¯•æ€»ç»“ï¼š")
    print(f"- å¹³å‡è¯¯å·®ï¼š{metrics_smooth['avg_error']:.2f}m")
    print(f"- 50mç²¾åº¦ï¼š{metrics_smooth['accuracy_50m']:.2f}%")
    print(f"- 80mç²¾åº¦ï¼š{metrics_smooth['accuracy_80m']:.2f}%")
    print(f"- æ–¹å‘ç¨³å®šæ€§ï¼š{direction_stability_smooth:.3f}")
    print("=" * 60)
    
    if metrics_smooth['avg_error'] < 60 and metrics_smooth['accuracy_50m'] > 70:
        print("ğŸ† åŸå§‹æ¨¡å‹æ€§èƒ½è‰¯å¥½ï¼")
    else:
        print("âš ï¸ åŸå§‹æ¨¡å‹æœ‰æ”¹è¿›ç©ºé—´")

if __name__ == "__main__":
    main()
